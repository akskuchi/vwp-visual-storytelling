[![CC BY license](https://img.shields.io/badge/License-CC%20BY-lightgray.svg)](https://creativecommons.org/licenses/by/4.0/)
[![Python](https://img.shields.io/badge/python-3.11-gold.svg)](https://www.python.org/downloads/release/python-311/)
[![PyTorch](https://img.shields.io/badge/Pytorch-2.0-pumpkin.svg)](https://pytorch.org/get-started/previous-versions/#v200)
[![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97-Models-purple)](https://huggingface.co/models?pipeline_tag=image-text-to-text&sort=trending)

# ðŸ‘€ What?
This repository contains code for implementing models and evaluation them for the Visual Storytelling task (using the VWP dataset):  
**[On the Challenges in Evaluating Visually Grounded Stories]()**&mdash;In proceedings of the [Text2Story](https://text2story25.inesctec.pt/) workshop (ECIR 2025).

**Note:** Despite being proposed specifically for visual storytelling, this method is generalizable and can be extended to any task involving model-generated outputs with corresponding references.

# ðŸ¤” Why?


# ðŸ¤– How?


---
ðŸ”— If you find this work useful, please consider citing it:
```
@inproceedings{
}
```
